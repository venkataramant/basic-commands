# Basics
    HDFS Distributed File System
    Spark Distributed Compute Framework
        Parallelism
        Optimizations
        Cache
        YARN

    
    HDFS - Alernatives
        Amazon S3,GCS,Azuer Blob
    
    Cloudera Sanbbox -- HortonWorks

# Formats
    Raw Text (Blobs) 
    Structured text files (CSV,XML, JSON)
    Sequeuence files
    AVRO
        Language-neutral data serialization
        Row Format
        Self-describing Schema Support
        Compressible,Splittable
    ORC (Row Columnar FaceBook)
    PARQUET (Inspired by Google)
        Columnar format
        Read-only selected columns
        Schema Support
        Compressible (Column level) and Splittable
        Supports nested data structures.
        Analytics use-cases [Performance and flexibility]
    
    ORC vs Parquet
        Nested Data (ORC < Parquet)
        Predicate Pushdown Efficiency (ORC > Parquet)
            Min Max + Bloomfilters
        ACID support (ORC)
        Compression (ORC > Parquet)
        Industry Support (Parquet > ORC)
        
# Compressions
    Snappy (Google) - Entire file [More Compression and Good performance]
    LZO - Splittable (License)
    GZIP (Very Good Compression and Moderate Processing) [Parquet]
    BZIP2 (Excellent Compression,Splittable, Slower Processing) [Backup/Snapshot]
# Partitioning Data
    Partitioning provides a way to read only a subset of data by providing a/set of partition key(s)
    Limited list of values
    Have to use them in Where conditions

# Bucketing
    Subdirectories based on a hash value are generated based on an attribute
    Limited no of unique directories and Even Distribution

# Use-Cases
    Heavy Read/Write
    What are Select filters
    compressions and optimizations
    partitioning and bucketings

# pyspark

```python

val rData   =  spark.read.option("inferSchema","true")
                .option("header","true")
                .csv("path of csv")
rData.printSchema()
rData.show(5)

rData.write.format("parquet").mode("overwrite").option("compression","gzip").save("/path")
rData.write.format("parquet").mode("overwirte").option("compression","gzip").partitionBy("c_name").save("path2")
rData.write.format("parquet").mode("overwrite").bucketBy(3,"Product").saveAsTable("table_name123")
# saveAsTable ...save it as HIVE Table
sc.getConf.get("spark.sql.warehouse.dir")
sql("SELECT * FROM table_name123 where product='Mouse'").show(5)
spark.time()
```
# Ingestion
    Enable Parallelism for Maximum write performance (Parquet)
        JDBC, Kafka Break down large files into smaller files
    Use APPEND for incremental data ingestion
   Spark OPtimizer -- For Performance and resources (Reduced I/O, Reduced shuffling, memory usage)
   Lazy Execution  - only an action triggers execution (Reduce/Collect)

# Best Practices
    Reduce data
        Filters + Required Columns
    Parallism supported data-sources/file formats
    Keep limited number of partitions. (Nof of Executiors x No of Cores)

# Optimizations

    - Projection Push Down
    - Pushing Down Filters
```python
val sData=spark.read.option("basePath","basedir").parquet("path/* or product=Pvaalue")
sData.select("cName1","cName2").explain # Projection Push Down

```

```python
val sData=spark.read.option("baseDir","baseDir1").parquet("pathofParquet")
dfJob=sData.where($"Product" === "PValue")
dfJob.expalin
```

# Partitioning and Coalescing

    Repartitioning does a full reshuffle and can be used for increasing/decreasing partitions (Wide)
    Coalescing - consolidates existing partitions and avoids a full reshuffle, used to decrease partitions (Narrow)

    Minimize no of shuffle
    Do actions late /after the filter
    Use aggregations by partition keys as much as possible, as records with same partittion keys stays in same execution mnode.

    spark.sql.autoBroadcatJoinThreshold

    Cache 
        - In Memory
            persist method
        - In Disk
            persist method
        Yarn -- Disk (By Default)
